# Elements of Production Deployed R Code  

R has a tricky historical reputation as a programming language. With its origins
in academia it is often argued that it is not fit for production deployment.  
The recent development of tools to support the deployment of R workloads are changing this. 
Below we will explore some key technical elements that should be included into any stable deployment of a data science
worklow adapted from @mlops. 

- Orchestration  
- Automation  
- Reproducibility  
- Version Control  
- Metadata and Documentation  
- Testing & Monitoring  

## Orchestration    

Orchestration refers to how your code is structured and run. We explored various ways of structuring our code for data science workflows in chapter two. There are two key facets to code orchestration. The first is separating the functional components of your project, and the orchestration of those functional components to do some task.  

As R is a functional programming language the canonical way to structure our code is to use functions. In a practical
sense, functions are a convenient way to package, document, test and control the execution of code in a project. 

Once the functional elements of your project are stored, documented and tested appropriately they need to be run in the correct order. This can be achieved using a number of frameworks (see X) such as using notebooks, an R Package, targets 
and more. 

## Automation   

Automation refers to how your project is 'built'. In other words how it is pushed and pulled from your local development environment into a remote environment where other users can access the results. Again, many options exist and there are varying levels of automation. Manual approaches such as click button deployment from your IDE or manually copying files across to a remote server are one option. 

```{mermaid}
graph LR
a1[Local Compute]-->a2[Server]<-->a3[User]
```

A better approach would be to stage your code using a remote version control repository. This
will act as a store or source code which can be more conveniently 'pulled' or synchronised with
the server where your analysis is hosted for users. Often, accompanying model artefacts or data will
also to stored somewhere remotely for the server to access.  

```{mermaid}
graph LR
a1[Local Compute]-->a2[Code Repo]
a1[Local Compute]-->a3[(Data Store)]
a2[Code Repo]-->a4[Server]
a3[(Data Store)]-->a4[Server]
a4[Server]<-->a5[User]
```


More contemporary approaches involve Continuous Integration and Continuous Deployment (CI/CD) practices. This is a DevOps style workflow that will automatically build, test and deploy code that has been pushed to a remote version control repository. A thorough exploration of CI/CD Solutions is beyond the scope of this book and is now a clearly defined sub specialty known as MLOps.  

```{mermaid}
graph LR
a1[Local Compute]-->a2[Code Repo]
 subgraph id1 [DevOps Pipeline]
  a2[[Code Repo]]-->b1[[Build]]
  b1[[Build]]-->b2[[Test]]
  b2[[Test]]-->b3[[Deploy to Container Registry]]
  end
a1[Local Compute]-->a3[(Data Store)]
b3[[Deploy to Container Registry]]-->a4[Server/Serverless App]
a3[(Data Store)]-->a4[Server]
a4[Server]<-->a5[User]
style id1 fill:lightblue;
```

## Reproducibility   

### Code dependencies  

We talk about reproducibility, there are many elements of a data science workflow that need to be considered Code dependencies. The first step to a reproducible pipeline is ensuring that all users have. The same code that is being used to run the project. The recommended approach here is to ensure that all code is checked in to a remote version control repository, using a version control system such as git.

Is that why other collaborators can clone the code base from the remote git repository? And, Branch. Or Fork from the code repository in order to make their own changes, it can then be integrated back. Using proper Version Control principles. Two users running the same code may not have the same R packages installed on this system.


### Packages dependencies  

A key element of reproducibility is ensuring that all users. Can resolve. Our package dependencies this includes having the correct packages. Installing those packages from the correct locations. And ensuring the version of those packages is equivalent. A convenient solution for these problems is the RN package. The RN package does this?


### System dependencies

System dependencies. System dependencies describe software, that is installed on. The computational environment that the project is being run on. These may include. External libraries. Such as Example. Tools like deposit, public package manager can provide some analysis of the system dependencies. That are required to support our packages on various operating systems.

Another solution explored in the next section. Is using Technologies, such as Docker. Operating system dependencies. Users, even when running the same code with the same, Our packages may find differences in how The code performs based on the operating system they're using, for example, Windows versus Linux versus Mac OS.


### OS dependencies  

It is common in a production setting to deploy code. To an external server using. A Linux operating system. A way to control operating system and system dependencies. Is. Use Technologies such as Docker, Docker does this. A basic Docker file is shown below. Finally. The dependencies. 

### Hardware dependencies  

Hardware dependencies are a little trickier. These are physical Hardware infrastructure constraints on how a project is run. Regardless of the operating system and software that is running on it. For example, this is commonly seen with the use of CPU versus GPU Technologies and different types of processor chips. A thorough exploration of Hardware dependencies is outside. The scope of this guide.

## Version Control  

Version Control. Version Control is a critical aspect. To ensure. Reproducibility instability in production deployed data, science Solutions. Version control includes. Not only Version Control for code, but also Version Control for data and models. Version control for code is commonly achieved using the get Tool.

Git is a version control system that allows users to Do this. If the data science project results in a Statistical and machine learning model being fit. It is the model itself that will be the deployed artefact in order for inference to be performed. Therefore it is critical that this model B also versioned. Again, there are many solutions for this. However, a recent development in, the r ecosystem is the vetivert package. A bit of a package, does this? The exploration of Version Control with data is beyond the scope of this book. However, It is recommended. That data is also versioned and stored appropriately in a secured data store, such as a data Lake or data warehouse.

## Metadata and Documentation   

Metadata and documentation. It is important. When deploying artefacts into a production setting, that there is appropriate metadata. Metadata refers to. Tags versions dates and other relevant information to describe what work is being deployed. We'll explore this further. In a modelling context, using the bit of a package. In terms of metadata and documentation for the functional aspects of the code, We can rely on the Internal documentation used in our packages.

Based on the oxygen package. This provides a useful template for documenting, our functions using oxygen tags. That are automatically generated into help documents. Another form of documentation in an R package is a readme. The package readme is an important artefact to tell users, what? Software does how it is to be run in any other important information such as the licence or prerequisites or dependencies.

Also how to contribute and get help? Low form documentation.

 
## Testing  


Testing. Finally. Test unit testing is important. Element of software engineering. In terms of our code. Testing, can be performed using the test that package. The test that package does this. And here is an example of it. Conveniently the test. That package is integrated into the build process for our packages.

Using the L Studio IDE. Testing can also be performed on a directory. Using this function.